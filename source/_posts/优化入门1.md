---
title: Introduction to Optimization 
date: 2024-10-23
tags:
  - 统计
  - 笔记
categories: 统计
description: 年轻人的第一门优化课。
---

来自《统计计算与软件》，2024年秋。

# Intro

## 优化算法的目标和特性

1. **鲁棒性（Robustness）**：算法应该在其类别的广泛问题上表现良好，对于所有合理的起始点值都能有效。
2. **效率（Efficiency）**：算法不应需要过多的计算机时间或存储空间。
3. **准确性（Accuracy）**：算法应能够精确地识别解决方案，而不应过于敏感于数据中的错误或在计算机上实现算法时发生的算术舍入错误。
4. **权衡（Tradeoffs）**：在收敛速度与存储需求之间、鲁棒性与速度之间等的权衡是数值优化中的中心问题。

## 优化与方程求解的关系

1. **方程求解转化为最小化问题**：求解方程 \( f(x) = 0 \) 可以转化为最小化问题 \( \min_{x} f(x)^T f(x) \)。
2. **最小化问题转化为方程求解**：最小化可微函数 \( f(x) \) 可以转化为求解 \( f'(x) = 0 \)。
3. **二次型函数的最小化**：当 \( f(x) \) 是二次型 \( f(x) = \frac{1}{2} x^T A x - b^T x + c \) 时，如果矩阵 \( A \) 是对称且正定的，那么 \( f(x) \) 可以通过解方程 \( Ax = b \) 来最小化。

## 梯度

梯度是一个向量，其分量是函数 \( f(x) \) 对每个变量的偏导数。对于一个多变量函数 \( f(x) \)，梯度表示为：
\[ \nabla f(x) = \left[ \frac{\partial}{\partial x_1} f(x), \frac{\partial}{\partial x_2} f(x), \ldots, \frac{\partial}{\partial x_n} f(x) \right] \]

1. 对于每一个点 \( x \)，梯度指向函数 \( f(x) \) 增加最快的方向。
    - 当 \( \Delta x \) 沿着梯度的方向时，函数 \( f(x) \) 的变化可以近似为：
    \[ f(x + \Delta x) \approx f(x) + (\Delta x)^T \nabla f(x) \]
    - 如果 \( \Delta x \) 取为 \( c \nabla f(x) \)，其中 \( c > 0 \)，那么 \( \Delta x \) 与等高线正交，这意味着在梯度方向上移动会使得函数值增加。

2. 梯度的正交补方向是函数 \( f(x) \) 保持不变的方向，即等高线（contour lines）的方向。

## e.g. MLE

### MLE的具体例子
1. **MLE的定义**：MLE是通过最大化对数似然函数 \( l(\theta) \) 来估计参数向量 \( \theta \) 的方法。这里，\( l(\theta) \) 是对数似然函数，\( \theta \) 是参数向量。
2. **Fisher得分方程**：通常，MLE \( \hat{\theta} \) 是Fisher得分方程的解，即 \( l'(\theta) = \frac{\partial}{\partial\theta} l(\theta) = 0 \)。
3. **优化与非线性方程**：寻找MLE相当于找到得分方程的根，这与解决非线性方程紧密相关。

### 轮廓MLE和集中MLE
1. **降维**：在高维参数空间中，通过将参数 \( \theta \) 分为感兴趣参数 \( \theta_1 \) 和干扰参数 \( \theta_2 \) 来降低问题的维度。
2. **轮廓MLE**：首先固定 \( \theta_1 \)，然后最大化 \( l(\theta_1, \theta_2) \) 来找到 \( \hat{\theta}_2(\theta_1) \)，接着最大化 \( l(\theta_1, \hat{\theta}_2(\theta_1)) \) 来找到 \( \hat{\theta}_1 \)。
3. **回归示例**：在回归分析中，如果 \( \beta_1 \) 和 \( \beta_2 \) 是参数，\( X_1 \) 和 \( X_2 \) 是设计矩阵，可以通过投影矩阵 \( P_2 \) 来分离 \( \beta_1 \) 和 \( \beta_2 \) 的估计。
4. **块松弛方法**：如果 \( \hat{\theta}_1 \) 也可以容易地作为 \( \theta_2 \) 的函数获得，可以使用块松弛方法，交替更新 \( \beta_1 \) 和 \( \beta_2 \) 的估计。

在处理高维参数时如何通过轮廓MLE和集中MLE来简化问题。
通过将参数分为感兴趣参数和干扰参数，可以更有效地进行参数估计。
此外，通过块松弛方法，可以交替更新参数估计，从而找到最优解。

### 相关术语

| 术语                  | 定义                                                                 |
|-----------------------|----------------------------------------------------------------------|
| 参数                  | \( \theta \)，一个p维向量                                             |
| 数据                  | \( X \)                                                               |
| 对数似然              | \( l(\theta) = \log P(X \mid \theta) \)                               |
| 得分函数              | \( s(\theta) = l'(\theta) = \left(\frac{\partial l}{\partial \theta_1}, \cdots, \frac{\partial l}{\partial \theta_p}\right)^T \) |
| 海森矩阵              | \( l''(\theta) = \left\{\frac{\partial^2 l}{\partial \theta_i \partial \theta_j}\right\}_{i, j=1,\cdots, p} \) |
| 费舍尔信息            | \( I(\theta) = -E[l''(\theta)] = E[J(\theta)] = E\left[l'(\theta)(l'(\theta))^T\right] \) |
| 观测信息              | \( J(\hat{\theta}) = -l''(\hat{\theta}) \)                             |
| 当 \( \theta^* \) 是局部最大值时 | \( l'(\theta^*) = 0 \)，且 \( l''(\theta^*) \) 是负定的                   |

## 迭代算法

**目标**：通过连续逼近来求解问题。

### 三要素

1. **初始值**：选择一个合理的初始猜测值，例如 \( x^{(0)} = 1.6 \)。
2. **更新规则**：从最近的值 \( x^{(t)} \) 产生一个改进的猜测 \( x^{(t+1)} \)，其中 \( t = 0, 1, 2, \ldots \)。
3. **停止条件**：确定何时停止迭代。

### 一些细节

#### 收敛准则

1. **绝对收敛准则**：
   - 停止条件：当 \(\left|x^{(t+1)} - x^{(t)}\right| < \varepsilon\) 时停止迭代。
   - 其中，\(\varepsilon\) 是一个常数，用于表示可接受的误差范围。

数值尺度？

2. **相对收敛准则**：
   - 停止条件：当 \(\frac{\left|x^{(t+1)} - x^{(t)}\right|}{\left|x^{(t)}\right|} < \varepsilon\) 时停止迭代。
   - 这个准则允许指定目标精度（例如，“在1%以内”），而无需考虑 \(x\) 的单位。

分母为0？

3. **另一种相对收敛监控方法**：
   - 停止条件：当 \(\frac{\left|x^{(t+1)} - x^{(t)}\right|}{\left|x^{(t)}\right| + \varepsilon} < \varepsilon\) 时停止迭代。

这些准则帮助确定迭代过程何时足够接近解，从而可以停止迭代，节省计算资源。
绝对收敛准则关注迭代值之间的绝对差异，而相对收敛准则则关注这种差异相对于当前迭代值的大小，后者在处理不同量级的问题时更为灵活。

#### 防治收敛失败

1. **固定迭代次数**：无论是否收敛，都可以在进行N次迭代后停止。
2. **收敛性监测**：如果任何收敛度量在几次迭代后没有减少或出现循环，或者解决方案本身循环不满意，可以考虑停止。
3. **避免劣化**：如果程序似乎正在收敛到一个点，而该点的函数值 \(f(x)\) 比你已经找到的另一个值要差，也应该停止。
4. **分散迭代资源**：不要将所有可用的迭代都用于一次尝试！应该为多次较小的尝试分配时间，以预期可能的收敛失败、数据修正、多个起始值等情况。

### 收敛速度

1. 定义
  - **定义**：设 \(\varepsilon^{(t)} \triangleq x^{(t)} - x^*\)，如果序列 \(\{x^{(t)}\}\) 收敛到 \(x^*\)，并且满足以下条件，则称其具有收敛阶数 \(\alpha\) 和收敛速度 \(c\)：
  \[
  \lim_{t \to \infty} \varepsilon^{(t)} = 0 \quad \text{和} \quad \lim_{t \to \infty} \frac{\|\varepsilon^{(t+1)}\|}{\|\varepsilon^{(t)}\|^\alpha} = c
  \]
  其中 \(c > 0\) 且 \(\alpha \geq 1\)。如果 \(\alpha > 1\)，则 \(c \in (0, \infty)\)；如果 \(\alpha = 1\)，则 \(c \in (0, 1)\)。

2. 例子
  - **线性收敛**：设 \(\gamma \in (0,1)\)，则 \(\gamma^n\) 线性收敛到零：
  \[
  \lim_{n \to \infty} \gamma^n = 0, \quad \frac{\gamma^{n+1}}{\gamma^n} = \gamma, \quad \gamma^{n+1} = O(\gamma^n)
  \]
  - **超线性收敛**：\(\gamma^{n^2}\) 超线性收敛到零：
  \[
  \lim_{n \to \infty} \frac{\gamma^{(n+1)^2}}{\gamma^{n^2}} = \lim_{n \to \infty} \gamma^{2n+1} = 0, \quad \lim_{n \to \infty} \frac{\gamma^{(n+1)^2}}{(\gamma^{n^2})^2} = \infty
  \]
  - **二次收敛**：\(\gamma^{2^n}\) 二次收敛到零：
  \[
  \frac{\gamma^{2^{n+1}}}{(\gamma^{2^n})^2} = 1
  \]

- 并非越快越好：存在tradeoff，快的可能不稳定。

# Direct Search Methods

应用情形：
1. 不可导
2. 导数计算过于复杂
3. 函数具有少数的变量

## 一元

### 黄金分割法 Golden Ratio/Section Search

1. 适用范围：$f(x)$ 在区间内**单峰**
2. 思路：选两个内点 -> 选一个内点完成 两头高中间低，一个内点作为新区间端点，一个内点在内 -> 要求按照统一的缩小比例 $r$ 追加一个内点 -> ...，保证可以循环则 $r$ 可以解出，为0.618。

3. 收敛阶数：1 （每次变成上次的r倍）

### 二次插值

1. 适用范围：连续函数
2. 思路：不断用二次函数最小值近似函数最小值
3. 收敛阶数高（快），1.324；不稳。*需要和黄金分割结合使用*

### R optimize Function

黄金分割法与二次插值的结合。

## 多元

### Nelder-Mead Method

R optim Function

# Gradient Methods

求导 -> 求根 $ f'(x) = 0 $  

### 图形法 Graphical Method

Plot and observe.

作用：
1. 确定根的大概位置/判断求根方法失败原因（没有根）
2. 结合迭代方法，为迭代法确定初值

### 二分法 Bisection Method

1. 适用条件：闭区间连续，两端异号
2. 思路：分割区间，得到区间的一半 -> 判断是否异号 -> 取异号一段，继续分割 -> ...

```{r}
bisec<-function(g, h = .05, b0, b1) {
  f <-function(x, h = h) {(g(x+h)-g(x-h))/2/h } #求导
  eps <-.Machine$double.eps
  r <-seq(b0, b1, length=3)
  y <-mapply(f, x = r, h = h)

  if (y[1] * y[3] > 0)
    stop("f does not have opposite  sign at endpoints")
  
  it <-0
  while(it < 1000 & abs(y[2]) > eps) {
    it <-it + 1
    if (y[1]*y[2] < 0) {
      r[3] <-r[2]y[3] <-y[2]
    } else {
      r[1] <-r[2]
      y[1] <-y[2]
    }
    r[2] <-(r[1] + r[3]) / 2
    y[2] <-f(r[2], h = h)
    print(c(r[2], y[2], y[3]-y[2]))
  }
  return(c(root = r[2], est= y[2], tol= y[3]-y[2]))
}
```

3. comments
  - 十分稳定，一定能找到**一个**根（多的会丢掉）
  - 慢，收敛阶数 1（一次减小一半，同阶）

### 开放法 Open Method

1. 思路：选取一个起始点 or 两个起始点不一定包含根 
2. 不一定收敛（不稳定）但快

#### 不动点迭代

\* 函数曲线越**平缓**（一阶导越小）越容易收敛/收敛越快。

- 简单理解：迭代时解不变->不动点，$G(x) = g'(x) +x \rightarrow G'(x) = g''(x) +1$
  - 万一不收敛？ *Scaled* Fixed-Point Iteration：$G(x) = \alpha g'(x) +x $
- 开放法构造迭代公式时依赖不动点原理。

#### 解线性方程组

对于问题 $Ax=b$
- $$x^{t+1} = G(x^{(t)})=Bx^{(t)}+d$$ 当$||b||<1$，收敛
- 当 矩阵A对角占优或正定，收敛
- Gauss-Seidel Method & SOR

#### 解非线性方程组

略

R uniroot Function
要求区间两端异号。 

#### 弦截法 Secant Interpolation

*属于开放法：可能收敛失败*

1. 两个起始点，迭代。
2. 迭代阶数：～1.618，超线性（很快）

